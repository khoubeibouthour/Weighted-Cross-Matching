screen
# screen -r

===========================================================================================================================================

ls()
rm(list = ls())

===========================================================================================================================================

grace
R

library("curl", lib.loc="~/work/RLibs")
library("httr", lib.loc="~/work/RLibs")
library("devtools", lib.loc="~/work/RLibs")
library("RCurl", lib.loc="~/work/RLibs")
library("h2o", lib.loc="~/work/RLibs")

h2o.init(nthreads = -1, min_mem_size="500G")
h2o.removeAll()

# h2o.clusterInfo()
# h2o.removeAll()
# h2o.shutdown(prompt=FALSE)
###################### if data is spread #########################################

Abrha = h2o.importFile(path="/home/kbouthou/work/dataset/Abrha")
Astraat = h2o.importFile(path="/home/kbouthou/work/dataset/Astraat")
Bonji = h2o.importFile(path="/home/kbouthou/work/dataset/Bonji")
Heeda = h2o.importFile(path="/home/kbouthou/work/dataset/Heeda")
Hickam = h2o.importFile(path="/home/kbouthou/work/dataset/Hickam")
Hmouda = h2o.importFile(path="/home/kbouthou/work/dataset/Hmouda")
Holu = h2o.importFile(path="/home/kbouthou/work/dataset/Holu")
Khufu = h2o.importFile(path="/home/kbouthou/work/dataset/Khufu")
Laghbesh = h2o.importFile(path="/home/kbouthou/work/dataset/Laghbesh")
Mekah = h2o.importFile(path="/home/kbouthou/work/dataset/Mekah")
Mimyth = h2o.importFile(path="/home/kbouthou/work/dataset/Mimyth")
Sakis = h2o.importFile(path="/home/kbouthou/work/dataset/Sakis")
Sierra = h2o.importFile(path="/home/kbouthou/work/dataset/Sierra")
Sokhoi = h2o.importFile(path="/home/kbouthou/work/dataset/Sokhoi")
Yakouza = h2o.importFile(path="/home/kbouthou/work/dataset/Yakouza")

dataset = h2o.rbind(Abrha, Astraat, Bonji, Heeda, Hickam, Hmouda, Holu, Khufu, Laghbesh, Mekah, Mimyth, Sakis, Sierra, Sokhoi, Yakouza)

####################### if data is put in one folder #############################

dataset = h2o.importFile(path="/home/kbouthou/work/dataset")

##################################################################################

dim(dataset)
h2o.str(dataset, cols=TRUE)
tab = h2o.table(dataset[,"response"])
h2o.print(tab, 2)

dataset$same_path = as.factor(dataset$same_path)
dataset$same_imagewidth = as.factor(dataset$same_imagewidth)
dataset$same_orientation = as.factor(dataset$same_orientation)
dataset$same_model = as.factor(dataset$same_model)
dataset$response = as.factor(dataset$response)

datasettt = h2o.splitFrame(data=dataset, ratios=0.8)


h2o.exportFile(data=datasettt[[1]], path="/home/kbouthou/work/dataset/s10/trainFolder", force=TRUE, parts=-1)
h2o.exportFile(data=datasettt[[2]], path="/home/kbouthou/work/dataset/s10/testFolder", force=TRUE, parts=-1)

========================================================================================================================

trainingSet = h2o.importFile(path="/home/kbouthou/work/dataset/s10/trainFolder")
trainingSet = trainingSet[,-c(4,5,6,7,8,9,10,11)]
trainingSet$response = as.factor(trainingSet$response)
h2o.str(trainingSet, cols=TRUE)

testSet = h2o.importFile(path="/home/kbouthou/work/dataset/s10/testFolder")
testSet = testSet[,-c(4,5,6,7,8,9,10,11)]
testSet$response = as.factor(testSet$response)

col.names = colnames(trainingSet)[-4]
col.names


trainedDRF = h2o.randomForest(
    x=col.names,
    model_id=NULL,
    training_frame=trainingSet,
    validation_frame=testSet,
    nfolds=10,
    y="response",
    keep_cross_validation_predictions=FALSE,
    keep_cross_validation_fold_assignment=FALSE,
    score_each_iteration=FALSE,
    score_tree_interval=0,
    fold_assignment="AUTO",
    fold_column=NULL,
    ignore_const_cols=TRUE,
    offset_column=NULL,
    weights_column=NULL,
    balance_classes=FALSE,
    class_sampling_factors=NULL,
    max_after_balance_size=1,
    max_hit_ratio_k=0,
    ntrees=5,
    max_depth=3,
    min_rows=1,
    nbins=20,
    nbins_top_level=1024,
    nbins_cats=1024,
    stopping_rounds=0,
    stopping_metric="AUTO",
    stopping_tolerance=0.001,
    max_runtime_secs=0,
    seed=-1,
    build_tree_one_node=FALSE,
    mtries=-1,
    sample_rate=0.6320000291,
    sample_rate_per_class=NULL,
    binomial_double_trees=TRUE,
    checkpoint=NULL,
    col_sample_rate_change_per_level=1,
    col_sample_rate_per_tree=1,
    min_split_improvement=1e-05,
    histogram_type="AUTO",
    categorical_encoding="AUTO",
    calibrate_model=FALSE
)
summary(trainedDRF)

h2o.confusionMatrix(trainedDRF)
h2o.auc(trainedDRF, valid=TRUE)
h2o.confusionMatrix(trainedDRF, newdata=testSet)

h2o.performance(trainedDRF, train=T)
h2o.performance(trainedDRF, valid=T)
h2o.performance(trainedDRF, newdata=train)
h2o.performance(trainedDRF, newdata=valid)
h2o.performance(trainedDRF, newdata=test)
h2o.hit_ratio_table(trainedDRF,valid = T)[1,2]

pred = h2o.predict(trainedDRF, test)
pred
test$Accuracy = pred$predict == test$response
1-mean(test$Accuracy)

========================================================================================================================


train = h2o.importFile(path="/home/kbouthou/work/dataset/s10/trainFolder")
train$same_path = as.factor(train$same_path)
train$same_imagewidth = as.factor(train$same_imagewidth)
train$same_orientation = as.factor(train$same_orientation)
train$same_model = as.factor(train$same_model)
train$response = as.factor(train$response)